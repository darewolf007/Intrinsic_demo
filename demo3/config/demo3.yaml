defaults:
    - tdmpc2
    - _self_
instrinsic_reward: true
# environment
seed: 3407
task: ms-peg-insertion-semi
obs: rgb
# obs: state
num_envs: 16

# RND Configuration
intrinsic_scale: 0.5      # RND 权重的最大值 (需要根据 Reward Scale 调整，建议 0.1 - 1.0)
rnd_out_dim: 128          # RND 特征维度
rnd_start_step: 500000    # 假设总步数 2M，从 500k 步之后才开始引入内在奖励
rnd_ramp_steps: 500000    # 用 500k 步的时间将权重从 0 拉升到 intrinsic_scale

# evaluation
eval_episodes: 10
eval_freq: 2000

# training
steps: 4_000_000
batch_size: 256
steps_per_update: 4 # utd = steps_per_update / num_envs (in this implementation)
buffer_size: 300_000
save_freq: 100_000
demo_sampling_ratio: 0.5 # Demonstration sampling ratio
policy_pretraining: true
use_demos: ???
seed_scheduler: false
demo_success_only: false

# pretraining
pretrain:
    n_epochs: 100000
    eval_freq: 5000
    log_freq: 500

# logging
wandb_project: instrinsic_demo3
wandb_entity: sun1599895936
wandb_silent: true
disable_wandb: false
save_csv: false

# reward learning
enable_reward_learning: true
n_stages: ???
demo_path: ??? #/path/to/demos/data.pkl
discriminator:
    disc_lr: 3e-4
    batch_size: 256
n_demos: 10
